
Multiple Testing
================

We'll begin by recreating the examples from this morning's lecture:

Coin Toss Experiments
---------------------

.. code:: python

    set.seed(231)
    x=sample(c("H","T"),10,replace=TRUE,prob=c(1/3,2/3)) # Create a sample of size 10, with probability of "H" = 1/3
                                                         # and probability of "T" = 2/3. Clearly, this is a biased coin.
    x




.. raw:: html

    <ol class=list-inline>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'H'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    </ol>




.. code:: python

    # Test for biasedness
    
    binom.test(sum(x=='T'), n=length(x), p = 0.5)




.. parsed-literal::

    
    	Exact binomial test
    
    data:  sum(x == "T") and length(x)
    number of successes = 9, number of trials = 10, p-value = 0.02148
    alternative hypothesis: true probability of success is not equal to 0.5
    95 percent confidence interval:
     0.5549839 0.9974714
    sample estimates:
    probability of success 
                       0.9 




So, our test concludes the coin is biased. Now we toss two coins:

.. code:: python

    set.seed(231)
    x1=sample(c("H","T"),10,replace=TRUE,prob=c(1/3,2/3))
    x2=sample(c("H","T"),10,replace=TRUE,prob=c(1/3,2/3))
    x1
    x2




.. raw:: html

    <ol class=list-inline>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'H'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    </ol>






.. raw:: html

    <ol class=list-inline>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'H'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    	<li>'T'</li>
    </ol>




.. code:: python

    # Test for bias
    binom.test(sum(x1=='T'), n=length(x), p = 0.5)
    





.. parsed-literal::

    
    	Exact binomial test
    
    data:  sum(x1 == "T") and length(x)
    number of successes = 9, number of trials = 10, p-value = 0.02148
    alternative hypothesis: true probability of success is not equal to 0.5
    95 percent confidence interval:
     0.5549839 0.9974714
    sample estimates:
    probability of success 
                       0.9 




.. code:: python

    binom.test(sum(x2=='T'), n=length(x), p = 0.5)




.. parsed-literal::

    
    	Exact binomial test
    
    data:  sum(x2 == "T") and length(x)
    number of successes = 9, number of trials = 10, p-value = 0.02148
    alternative hypothesis: true probability of success is not equal to 0.5
    95 percent confidence interval:
     0.5549839 0.9974714
    sample estimates:
    probability of success 
                       0.9 




.. code:: python

    ?binom.test





.. raw:: html

    
    <table width="100%" summary="page for binom.test {stats}"><tr><td>binom.test {stats}</td><td style="text-align: right;">R Documentation</td></tr></table>
    
    <h2>Exact Binomial Test</h2>
    
    <h3>Description</h3>
    
    <p>Performs an exact test of a simple null hypothesis about the
    probability of success in a Bernoulli experiment.
    </p>
    
    
    <h3>Usage</h3>
    
    <pre>
    binom.test(x, n, p = 0.5,
               alternative = c("two.sided", "less", "greater"),
               conf.level = 0.95)
    </pre>
    
    
    <h3>Arguments</h3>
    
    <table summary="R argblock">
    <tr valign="top"><td><code>x</code></td>
    <td>
    <p>number of successes, or a vector of length 2 giving the
    numbers of successes and failures, respectively.</p>
    </td></tr>
    <tr valign="top"><td><code>n</code></td>
    <td>
    <p>number of trials; ignored if <code>x</code> has length 2.</p>
    </td></tr>
    <tr valign="top"><td><code>p</code></td>
    <td>
    <p>hypothesized probability of success.</p>
    </td></tr>
    <tr valign="top"><td><code>alternative</code></td>
    <td>
    <p>indicates the alternative hypothesis and must be
    one of <code>"two.sided"</code>, <code>"greater"</code> or <code>"less"</code>.
    You can specify just the initial letter.</p>
    </td></tr>
    <tr valign="top"><td><code>conf.level</code></td>
    <td>
    <p>confidence level for the returned confidence
    interval.</p>
    </td></tr>
    </table>
    
    
    <h3>Details</h3>
    
    <p>Confidence intervals are obtained by a procedure first given in
    Clopper and Pearson (1934).  This guarantees that the confidence level
    is at least <code>conf.level</code>, but in general does not give the
    shortest-length confidence intervals.
    </p>
    
    
    <h3>Value</h3>
    
    <p>A list with class <code>"htest"</code> containing the following components:
    </p>
    <table summary="R valueblock">
    <tr valign="top"><td><code>statistic</code></td>
    <td>
    <p>the number of successes.</p>
    </td></tr>
    <tr valign="top"><td><code>parameter</code></td>
    <td>
    <p>the number of trials.</p>
    </td></tr>
    <tr valign="top"><td><code>p.value</code></td>
    <td>
    <p>the p-value of the test.</p>
    </td></tr>
    <tr valign="top"><td><code>conf.int</code></td>
    <td>
    <p>a confidence interval for the probability of success.</p>
    </td></tr>
    <tr valign="top"><td><code>estimate</code></td>
    <td>
    <p>the estimated probability of success.</p>
    </td></tr>
    <tr valign="top"><td><code>null.value</code></td>
    <td>
    <p>the probability of success under the null,
    <code>p</code>.</p>
    </td></tr>
    <tr valign="top"><td><code>alternative</code></td>
    <td>
    <p>a character string describing the alternative
    hypothesis.</p>
    </td></tr>
    <tr valign="top"><td><code>method</code></td>
    <td>
    <p>the character string <code>"Exact binomial test"</code>.</p>
    </td></tr>
    <tr valign="top"><td><code>data.name</code></td>
    <td>
    <p>a character string giving the names of the data.</p>
    </td></tr>
    </table>
    
    
    <h3>References</h3>
    
    <p>Clopper, C. J. &amp; Pearson, E. S. (1934).
    The use of confidence or fiducial limits illustrated in the case of
    the binomial.
    <em>Biometrika</em>, <b>26</b>, 404&ndash;413.
    </p>
    <p>William J. Conover (1971),
    <em>Practical nonparametric statistics</em>.
    New York: John Wiley &amp; Sons.
    Pages 97&ndash;104.
    </p>
    <p>Myles Hollander &amp; Douglas A. Wolfe (1973),
    <em>Nonparametric Statistical Methods.</em>
    New York: John Wiley &amp; Sons.
    Pages 15&ndash;22.
    </p>
    
    
    <h3>See Also</h3>
    
    <p><code>prop.test</code> for a general (approximate) test for equal or
    given proportions.
    </p>
    
    
    <h3>Examples</h3>
    
    <pre>
    ## Conover (1971), p. 97f.
    ## Under (the assumption of) simple Mendelian inheritance, a cross
    ##  between plants of two particular genotypes produces progeny 1/4 of
    ##  which are "dwarf" and 3/4 of which are "giant", respectively.
    ##  In an experiment to determine if this assumption is reasonable, a
    ##  cross results in progeny having 243 dwarf and 682 giant plants.
    ##  If "giant" is taken as success, the null hypothesis is that p =
    ##  3/4 and the alternative that p != 3/4.
    binom.test(c(682, 243), p = 3/4)
    binom.test(682, 682 + 243, p = 3/4)   # The same.
    ## =&gt; Data are in agreement with the null hypothesis.
    </pre>
    
    <hr /><div style="text-align: center;">[Package <em>stats</em> version 3.2.0 ]</div>



.. code:: python

    # Suppose we toss 100 fair coins
    count.reject=0
    for (i in 1:100){
        x2=sample(c("H","T"),10,replace=TRUE,prob=c(1/2,1/2))
        result=binom.test(sum(x2=='T'), n=length(x2), p = 0.5)
        print(result$p.value)
        if (result$p.value<.05) {count.reject=count.reject+1}
        }
    count.reject


.. parsed-literal::

    [1] 0.34375
    [1] 1
    [1] 0.109375
    [1] 1
    [1] 1
    [1] 0.109375
    [1] 0.34375
    [1] 0.34375
    [1] 1
    [1] 0.34375
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.34375
    [1] 1
    [1] 0.02148438
    [1] 0.34375
    [1] 0.34375
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.34375
    [1] 0.7539063
    [1] 0.7539063
    [1] 1
    [1] 0.34375
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.34375
    [1] 0.7539063
    [1] 1
    [1] 0.34375
    [1] 0.34375
    [1] 1
    [1] 1
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.109375
    [1] 1
    [1] 0.34375
    [1] 0.7539063
    [1] 1
    [1] 0.34375
    [1] 1
    [1] 0.34375
    [1] 0.34375
    [1] 0.7539063
    [1] 0.109375
    [1] 1
    [1] 0.109375
    [1] 1
    [1] 1
    [1] 1
    [1] 0.7539063
    [1] 0.34375
    [1] 1
    [1] 0.109375
    [1] 0.7539063
    [1] 0.7539063
    [1] 1
    [1] 0.7539063
    [1] 0.109375
    [1] 0.34375
    [1] 0.7539063
    [1] 1
    [1] 0.7539063
    [1] 0.34375
    [1] 0.109375
    [1] 0.34375
    [1] 0.34375
    [1] 0.109375
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.7539063
    [1] 0.7539063
    [1] 1
    [1] 1
    [1] 0.34375
    [1] 0.7539063
    [1] 1
    [1] 1
    [1] 1
    [1] 1
    [1] 0.7539063
    [1] 0.34375
    [1] 0.02148438
    [1] 0.109375
    [1] 0.34375
    [1] 0.7539063
    [1] 0.34375
    [1] 0.7539063
    [1] 0.02148438
    [1] 0.34375
    [1] 0.34375
    [1] 1
    [1] 1
    [1] 1
    [1] 0.34375
    [1] 0.7539063
    [1] 0.7539063




.. raw:: html

    3



We can consider the number of tosses to be the 'sample size' and the
number of hypotheses tested to be the number of coins tossed. Here, our
sample size is small compared to the number of hypotheses tested. In
genome data, the sample size is the number of patients and the number of
hypotheses tested is the number of genes (or SNPs) analyzed. If we set a
significance level of :math:`0.05`, we are saying that we expect to find
a false positive 5% of the time. So in our coin toss, we would expect to
find the coin to be fair 5 times out of 100.

.. code:: python

    # What happens if we increase the number of tosses?

